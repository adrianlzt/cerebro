Large language models


# LangChain
https://github.com/hwchase17/langchain/
https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain

Librería para usar los LLMs con otras entradas/salidas. Combinaciones para sacarle más partido.


# LLaMA / Alpaca / LoRA
https://github.com/lxe/simple-llama-finetuner
https://github.com/facebookresearch/llama/pull/73
https://news.ycombinator.com/item?id=35107058&fbclid=IwAR3kdhbqOHfDezKZ_y-tUHGu-RcCzd_GEnmiiMQC6e2r57z78nyExNKI07M
LoRA is now supported by the State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) library by HuggingFace.

A team at Stanford gave LLaMA superpowers, by finetuning it on a synthetic instruction dataset into Stanford Alpaca to behave more like ChatGPT. They didn't release the weights either, but shared everything else needed to replicate it.

The team behind Point Network replicated this experiment and released the weights publicly

https://twitter.com/PointNetwork/status/1636980445274406912?t=KAp5QJn905c8fd4LIv-wbw&s=08

https://github.com/ggerganov/llama.cpp
https://github.com/antimatter15/alpaca.cpp
https://github.com/lxe/simple-llama-finetuner
