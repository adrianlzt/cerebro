<https://github.com/ollama/ollama/tree/main/docs>

<https://ollama.com/download>
pacman -S ollama
sc-start ollama

Por un par de pruebas, parece que consumen tanta memoria como el tama침o del fichero que hace falta descargar.

phi4: este se come 10GiB y va bastante lento. Pone la CPU a tope. Lento, pero parece razonablemente bueno con aider
deepseek-r1: el tema de razonar consume m치s tokens, y no es muy r치pido, por lo que se hace un poco lento

Arrancar un modelo, nos abre un chat para poder preguntarle.
Seguir치 arrancado si no lo paramos.

```bash
ollama run gemma:2b
```

Modelos corriendo:

```bash
ollama ps
```

Info modelo:

```bash
ollama show gemma:2b
```

Parar un modelo:

```bash
ollama stop gemma:2b
```

Cambiar el directorio para bajar los modelos: <https://github.com/ollama/ollama/blob/main/docs/faq.md#where-are-models-stored>
